{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Demo - Advanced Features\n",
    "\n",
    "This notebook demonstrates advanced features of the RAG system:\n",
    "- Hybrid retrieval (semantic + keyword)\n",
    "- Different chunking strategies\n",
    "- Streaming responses\n",
    "- Custom prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.pipeline.rag import RAGPipeline\n",
    "from src.embeddings.factory import create_embedding_provider\n",
    "from src.retrieval.factory import create_vector_store, create_retrieval_strategy\n",
    "from src.ingest.chunking import RecursiveChunker, ParagraphChunker\n",
    "from src.utils.config_loader import load_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Hybrid Retrieval\n",
    "\n",
    "Combine semantic search with keyword-based search for better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config and modify for hybrid retrieval\n",
    "config = load_config(\"../configs/config.yaml\")\n",
    "config[\"retrieval\"][\"strategy\"] = \"hybrid\"\n",
    "config[\"retrieval\"][\"config\"][\"semantic_weight\"] = 0.7\n",
    "config[\"retrieval\"][\"config\"][\"keyword_weight\"] = 0.3\n",
    "\n",
    "# Create pipeline with hybrid retrieval\n",
    "pipeline = RAGPipeline(config=config)\n",
    "\n",
    "query = \"How do I automate infrastructure provisioning?\"\n",
    "response = pipeline.query(query)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(f\"Answer: {response['answer']}\\n\")\n",
    "print(f\"Retrieved {len(response['sources'])} documents using hybrid search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Different Chunking Strategies\n",
    "\n",
    "Compare results from different chunking strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "Terraform is a powerful infrastructure as code tool. It allows you to define\n",
    "infrastructure in configuration files. These files use HCL syntax.\n",
    "\n",
    "Terraform modules help organize your code. A module is a container for multiple\n",
    "resources that are used together. Modules are the main way to package and reuse\n",
    "resource configurations in Terraform.\n",
    "\n",
    "To create a module, you need three main files: variables.tf for input variables,\n",
    "main.tf for resource definitions, and outputs.tf for output values.\n",
    "\"\"\"\n",
    "\n",
    "# Test different chunking strategies\n",
    "strategies = {\n",
    "    \"Recursive\": RecursiveChunker(chunk_size=100, chunk_overlap=20),\n",
    "    \"Paragraph\": ParagraphChunker(max_chunk_size=200)\n",
    "}\n",
    "\n",
    "for name, chunker in strategies.items():\n",
    "    chunks = chunker.chunk_text(sample_text)\n",
    "    print(f\"\\n{name} Chunking: {len(chunks)} chunks\")\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        print(f\"  Chunk {i} ({len(chunk)} chars): {chunk[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Streaming Responses\n",
    "\n",
    "Stream LLM responses token by token for better UX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "\n",
    "pipeline = RAGPipeline(config_path=\"../configs/config.yaml\")\n",
    "\n",
    "query = \"Explain what Ansible playbooks are used for\"\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Streaming response:\\n\")\n",
    "\n",
    "# Stream the response\n",
    "full_response = \"\"\n",
    "for chunk in pipeline.query_stream(query):\n",
    "    full_response += chunk\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\\nâœ“ Streaming complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Adjusting Retrieval Parameters\n",
    "\n",
    "Fine-tune retrieval for your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different top_k values\n",
    "query = \"How do I use infrastructure as code?\"\n",
    "\n",
    "for top_k in [1, 3, 5]:\n",
    "    config = load_config(\"../configs/config.yaml\")\n",
    "    config[\"retrieval\"][\"config\"][\"top_k\"] = top_k\n",
    "    \n",
    "    pipeline = RAGPipeline(config=config)\n",
    "    response = pipeline.query(query)\n",
    "    \n",
    "    print(f\"\\ntop_k={top_k}: Retrieved {len(response['sources'])} documents\")\n",
    "    print(f\"Answer length: {len(response['answer'])} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Examining Retrieved Context\n",
    "\n",
    "Inspect what documents are being retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = RAGPipeline(config_path=\"../configs/config.yaml\")\n",
    "\n",
    "query = \"What tools are available for configuration management?\"\n",
    "response = pipeline.query(query)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Retrieved Context:\\n\")\n",
    "\n",
    "for i, source in enumerate(response['sources'], 1):\n",
    "    print(f\"\\nDocument {i}:\")\n",
    "    print(f\"  Source: {source['metadata'].get('source', 'unknown')}\")\n",
    "    print(f\"  Score: {source['score']:.4f}\")\n",
    "    print(f\"  Content: {source['content'][:150]}...\")\n",
    "\n",
    "print(f\"\\n\\nGenerated Answer:\\n{response['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Comparison\n",
    "\n",
    "Compare different retrieval strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "query = \"How do I deploy applications with automation?\"\n",
    "strategies = [\"semantic\", \"hybrid\"]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for strategy in strategies:\n",
    "    config = load_config(\"../configs/config.yaml\")\n",
    "    config[\"retrieval\"][\"strategy\"] = strategy\n",
    "    \n",
    "    pipeline = RAGPipeline(config=config)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = pipeline.query(query)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    results[strategy] = {\n",
    "        \"time\": elapsed,\n",
    "        \"sources\": len(response['sources']),\n",
    "        \"answer_length\": len(response['answer'])\n",
    "    }\n",
    "\n",
    "print(\"Performance Comparison:\\n\")\n",
    "for strategy, metrics in results.items():\n",
    "    print(f\"{strategy.capitalize()}:\")\n",
    "    print(f\"  Time: {metrics['time']:.2f}s\")\n",
    "    print(f\"  Sources retrieved: {metrics['sources']}\")\n",
    "    print(f\"  Answer length: {metrics['answer_length']} chars\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- Explore `03_custom_components.ipynb` for component customization\n",
    "- Experiment with different embedding models\n",
    "- Try different LLM providers (OpenAI, Anthropic)\n",
    "- Tune parameters for your specific use case"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
